{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1952460b",
   "metadata": {},
   "source": [
    "# GenAI Developer Assignment ‚Äî Travel Assistant (LangGraph + Gemini)\n",
    "\n",
    "This notebook contains the **complete implementation** with comprehensive logging.\n",
    "\n",
    "---\n",
    "## Objective\n",
    "Build an intelligent **Travel Assistant** using:\n",
    "- **Gemini API (Flash / Pro)**\n",
    "- **Tools**: `search_flights`, `get_weather`, `find_attractions`\n",
    "- **FastAPI endpoint** (`/travel-assistant`)\n",
    "- **Retry logic with exponential backoff**\n",
    "- **Streaming responses** for better UX\n",
    "- **LangGraph framework**\n",
    "- **Comprehensive logging mechanism**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ca4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TASK 0: Setup and Dependencies\n",
    "# ============================================\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import asyncio\n",
    "import time\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from functools import wraps\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# FastAPI imports\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Other imports\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "# ============================================\n",
    "# LOGGING SETUP\n",
    "# ============================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('travel_assistant.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"üöÄ Travel Assistant Application Starting...\")\n",
    "logger.info(\"üì¶ All dependencies imported successfully\")\n",
    "print(\"‚úÖ Setup complete! All imports successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7afca961",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Configure Gemini API\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlogger\u001b[49m.info(\u001b[33m\"\u001b[39m\u001b[33müìù Loading environment variables...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m load_dotenv()\n\u001b[32m      5\u001b[39m GOOGLE_API_KEY = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "# Configure Gemini API\n",
    "logger.info(\"üìù Loading environment variables...\")\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    logger.error(\"‚ùå GOOGLE_API_KEY not found in environment variables\")\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Please set it in .env file\")\n",
    "\n",
    "logger.info(\"üîë API key found\")\n",
    "\n",
    "# Configure Gemini\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "logger.info(\"‚úÖ Gemini API configured\")\n",
    "\n",
    "# Initialize Gemini model\n",
    "MODEL_NAME = \"gemini-1.5-flash\"\n",
    "logger.info(f\"ü§ñ Initializing model: {MODEL_NAME}\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "logger.info(\"‚úÖ LLM model initialized successfully\")\n",
    "print(f\"‚úÖ Gemini API configured with model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8cbbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement mock tools with comprehensive logging\n",
    "\n",
    "@tool\n",
    "def search_flights(origin: str, destination: str, date: str = \"2025-12-01\") -> dict:\n",
    "    \"\"\"\n",
    "    Search for flight options between origin and destination.\n",
    "    \n",
    "    Args:\n",
    "        origin: Departure city\n",
    "        destination: Arrival city\n",
    "        date: Travel date (YYYY-MM-DD format)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing flight options\n",
    "    \"\"\"\n",
    "    logger.info(f\"üõ´ search_flights called: {origin} ‚Üí {destination} on {date}\")\n",
    "    \n",
    "    # Mock flight data\n",
    "    mock_flights = {\n",
    "        \"flights\": [\n",
    "            {\n",
    "                \"airline\": \"Singapore Airlines\",\n",
    "                \"flight_number\": \"SQ638\",\n",
    "                \"price_usd\": 450,\n",
    "                \"departure_time\": \"07:00 AM\",\n",
    "                \"arrival_time\": \"02:30 PM\",\n",
    "                \"duration\": \"6h 30m\",\n",
    "                \"stops\": \"Direct\"\n",
    "            },\n",
    "            {\n",
    "                \"airline\": \"ANA\",\n",
    "                \"flight_number\": \"NH842\",\n",
    "                \"price_usd\": 420,\n",
    "                \"departure_time\": \"11:30 AM\",\n",
    "                \"arrival_time\": \"07:00 PM\",\n",
    "                \"duration\": \"6h 30m\",\n",
    "                \"stops\": \"Direct\"\n",
    "            },\n",
    "            {\n",
    "                \"airline\": \"JAL\",\n",
    "                \"flight_number\": \"JL712\",\n",
    "                \"price_usd\": 480,\n",
    "                \"departure_time\": \"09:15 AM\",\n",
    "                \"arrival_time\": \"04:45 PM\",\n",
    "                \"duration\": \"6h 30m\",\n",
    "                \"stops\": \"Direct\"\n",
    "            }\n",
    "        ],\n",
    "        \"origin\": origin,\n",
    "        \"destination\": destination,\n",
    "        \"date\": date\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"‚úÖ Found {len(mock_flights['flights'])} flights from {origin} to {destination}\")\n",
    "    logger.debug(f\"Flight data: {json.dumps(mock_flights, indent=2)}\")\n",
    "    \n",
    "    return mock_flights\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str, days: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Get weather forecast for a location.\n",
    "    \n",
    "    Args:\n",
    "        location: City name\n",
    "        days: Number of days for forecast (1-7)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing weather forecast\n",
    "    \"\"\"\n",
    "    logger.info(f\"üå§Ô∏è  get_weather called: {location} for {days} days\")\n",
    "    \n",
    "    # Mock weather conditions\n",
    "    conditions = [\"Sunny\", \"Partly Cloudy\", \"Cloudy\", \"Light Rain\", \"Clear\"]\n",
    "    \n",
    "    forecast = {\n",
    "        \"location\": location,\n",
    "        \"forecast\": []\n",
    "    }\n",
    "    \n",
    "    for i in range(min(days, 7)):\n",
    "        day_forecast = {\n",
    "            \"day\": f\"Day {i + 1}\",\n",
    "            \"condition\": conditions[i % len(conditions)],\n",
    "            \"temperature_celsius\": 22 + (i % 5),\n",
    "            \"humidity_percent\": 55 + (i * 5),\n",
    "            \"precipitation_chance\": 10 + (i * 5)\n",
    "        }\n",
    "        forecast[\"forecast\"].append(day_forecast)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Weather forecast retrieved for {location} ({days} days)\")\n",
    "    logger.debug(f\"Weather data: {json.dumps(forecast, indent=2)}\")\n",
    "    \n",
    "    return forecast\n",
    "\n",
    "\n",
    "@tool\n",
    "def find_attractions(location: str, limit: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Find top tourist attractions in a location.\n",
    "    \n",
    "    Args:\n",
    "        location: City name\n",
    "        limit: Maximum number of attractions to return\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing attraction information\n",
    "    \"\"\"\n",
    "    logger.info(f\"üóº find_attractions called: {location} (limit: {limit})\")\n",
    "    \n",
    "    # Mock attractions database\n",
    "    attractions_db = {\n",
    "        \"Tokyo\": [\n",
    "            {\"name\": \"Shibuya Crossing\", \"type\": \"Landmark\", \"rating\": 4.8, \"description\": \"Famous scramble crossing\"},\n",
    "            {\"name\": \"Senso-ji Temple\", \"type\": \"Temple\", \"rating\": 4.7, \"description\": \"Ancient Buddhist temple in Asakusa\"},\n",
    "            {\"name\": \"Tokyo Skytree\", \"type\": \"Observation Tower\", \"rating\": 4.6, \"description\": \"Tallest structure in Japan\"},\n",
    "            {\"name\": \"Meiji Shrine\", \"type\": \"Shrine\", \"rating\": 4.7, \"description\": \"Shinto shrine in forest setting\"},\n",
    "            {\"name\": \"Tokyo Tower\", \"type\": \"Landmark\", \"rating\": 4.5, \"description\": \"Iconic communications tower\"},\n",
    "            {\"name\": \"Tsukiji Outer Market\", \"type\": \"Market\", \"rating\": 4.6, \"description\": \"Fresh seafood and street food\"},\n",
    "        ],\n",
    "        \"default\": [\n",
    "            {\"name\": \"City Center\", \"type\": \"District\", \"rating\": 4.5, \"description\": \"Main downtown area\"},\n",
    "            {\"name\": \"Historic Quarter\", \"type\": \"District\", \"rating\": 4.6, \"description\": \"Old town with traditional architecture\"},\n",
    "            {\"name\": \"Central Park\", \"type\": \"Park\", \"rating\": 4.4, \"description\": \"Large urban park\"},\n",
    "            {\"name\": \"National Museum\", \"type\": \"Museum\", \"rating\": 4.7, \"description\": \"Cultural and historical exhibits\"},\n",
    "            {\"name\": \"Waterfront\", \"type\": \"Area\", \"rating\": 4.5, \"description\": \"Scenic riverside or harbor area\"},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Get attractions for location or use default\n",
    "    attractions_list = attractions_db.get(location, attractions_db[\"default\"])\n",
    "    limited_attractions = attractions_list[:limit]\n",
    "    \n",
    "    result = {\n",
    "        \"location\": location,\n",
    "        \"attractions\": limited_attractions,\n",
    "        \"total_found\": len(limited_attractions)\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"‚úÖ Found {len(limited_attractions)} attractions in {location}\")\n",
    "    logger.debug(f\"Attractions data: {json.dumps(result, indent=2)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Register tools\n",
    "tools = [search_flights, get_weather, find_attractions]\n",
    "logger.info(f\"üìã Registered {len(tools)} tools: {[t.name for t in tools]}\")\n",
    "\n",
    "print(\"‚úÖ Tools implemented successfully!\")\n",
    "print(f\"Available tools: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement retry logic wrapper with comprehensive logging\n",
    "\n",
    "def retry_with_exponential_backoff(\n",
    "    max_retries: int = 3,\n",
    "    initial_delay: float = 1.0,\n",
    "    exponential_base: float = 2.0,\n",
    "    max_delay: float = 60.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Decorator for retrying a function with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        initial_delay: Initial delay in seconds\n",
    "        exponential_base: Base for exponential calculation\n",
    "        max_delay: Maximum delay between retries\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        async def async_wrapper(*args, **kwargs):\n",
    "            retry_count = 0\n",
    "            \n",
    "            while retry_count <= max_retries:\n",
    "                try:\n",
    "                    logger.debug(f\"üîÑ Attempting {func.__name__} (attempt {retry_count + 1}/{max_retries + 1})\")\n",
    "                    result = await func(*args, **kwargs)\n",
    "                    \n",
    "                    if retry_count > 0:\n",
    "                        logger.info(f\"‚úÖ {func.__name__} succeeded after {retry_count} retries\")\n",
    "                    \n",
    "                    return result\n",
    "                    \n",
    "                except (google_exceptions.ResourceExhausted,\n",
    "                        google_exceptions.ServiceUnavailable,\n",
    "                        google_exceptions.DeadlineExceeded,\n",
    "                        ConnectionError,\n",
    "                        TimeoutError) as e:\n",
    "                    \n",
    "                    retry_count += 1\n",
    "                    \n",
    "                    if retry_count > max_retries:\n",
    "                        logger.error(f\"‚ùå {func.__name__} failed after {max_retries} retries: {str(e)}\")\n",
    "                        raise\n",
    "                    \n",
    "                    # Calculate delay with exponential backoff\n",
    "                    delay = min(initial_delay * (exponential_base ** (retry_count - 1)), max_delay)\n",
    "                    \n",
    "                    logger.warning(\n",
    "                        f\"‚ö†Ô∏è  {func.__name__} failed (attempt {retry_count}/{max_retries + 1}): {type(e).__name__}\"\n",
    "                    )\n",
    "                    logger.info(f\"üîÑ Retrying in {delay:.2f} seconds...\")\n",
    "                    \n",
    "                    await asyncio.sleep(delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Don't retry on non-transient errors\n",
    "                    logger.error(f\"‚ùå {func.__name__} failed with non-retryable error: {type(e).__name__}: {str(e)}\")\n",
    "                    raise\n",
    "            \n",
    "            logger.error(f\"‚ùå {func.__name__} exceeded maximum retries\")\n",
    "            raise Exception(f\"Maximum retries ({max_retries}) exceeded for {func.__name__}\")\n",
    "        \n",
    "        @wraps(func)\n",
    "        def sync_wrapper(*args, **kwargs):\n",
    "            retry_count = 0\n",
    "            \n",
    "            while retry_count <= max_retries:\n",
    "                try:\n",
    "                    logger.debug(f\"üîÑ Attempting {func.__name__} (attempt {retry_count + 1}/{max_retries + 1})\")\n",
    "                    result = func(*args, **kwargs)\n",
    "                    \n",
    "                    if retry_count > 0:\n",
    "                        logger.info(f\"‚úÖ {func.__name__} succeeded after {retry_count} retries\")\n",
    "                    \n",
    "                    return result\n",
    "                    \n",
    "                except (google_exceptions.ResourceExhausted,\n",
    "                        google_exceptions.ServiceUnavailable,\n",
    "                        google_exceptions.DeadlineExceeded,\n",
    "                        ConnectionError,\n",
    "                        TimeoutError) as e:\n",
    "                    \n",
    "                    retry_count += 1\n",
    "                    \n",
    "                    if retry_count > max_retries:\n",
    "                        logger.error(f\"‚ùå {func.__name__} failed after {max_retries} retries: {str(e)}\")\n",
    "                        raise\n",
    "                    \n",
    "                    # Calculate delay with exponential backoff\n",
    "                    delay = min(initial_delay * (exponential_base ** (retry_count - 1)), max_delay)\n",
    "                    \n",
    "                    logger.warning(\n",
    "                        f\"‚ö†Ô∏è  {func.__name__} failed (attempt {retry_count}/{max_retries + 1}): {type(e).__name__}\"\n",
    "                    )\n",
    "                    logger.info(f\"üîÑ Retrying in {delay:.2f} seconds...\")\n",
    "                    \n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Don't retry on non-transient errors\n",
    "                    logger.error(f\"‚ùå {func.__name__} failed with non-retryable error: {type(e).__name__}: {str(e)}\")\n",
    "                    raise\n",
    "            \n",
    "            logger.error(f\"‚ùå {func.__name__} exceeded maximum retries\")\n",
    "            raise Exception(f\"Maximum retries ({max_retries}) exceeded for {func.__name__}\")\n",
    "        \n",
    "        # Return appropriate wrapper based on function type\n",
    "        if asyncio.iscoroutinefunction(func):\n",
    "            return async_wrapper\n",
    "        else:\n",
    "            return sync_wrapper\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "\n",
    "logger.info(\"‚úÖ Retry logic with exponential backoff implemented\")\n",
    "print(\"‚úÖ Retry logic implemented with exponential backoff (1s, 2s, 4s, 8s...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement streaming response handler with comprehensive logging\n",
    "\n",
    "async def stream_llm_response(messages: list, tools_list: list = None):\n",
    "    \"\"\"\n",
    "    Stream responses from Gemini LLM.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        tools_list: Optional list of tools to bind\n",
    "    \n",
    "    Yields:\n",
    "        Partial response chunks\n",
    "    \"\"\"\n",
    "    logger.info(\"üì° Starting streaming LLM response\")\n",
    "    logger.debug(f\"Input messages count: {len(messages)}\")\n",
    "    \n",
    "    try:\n",
    "        # Bind tools if provided\n",
    "        model = llm\n",
    "        if tools_list:\n",
    "            model = llm.bind_tools(tools_list)\n",
    "            logger.debug(f\"Tools bound to model: {[t.name for t in tools_list]}\")\n",
    "        \n",
    "        # Stream the response\n",
    "        chunk_count = 0\n",
    "        full_response = \"\"\n",
    "        \n",
    "        logger.info(\"üöÄ Invoking LLM with streaming...\")\n",
    "        async for chunk in model.astream(messages):\n",
    "            chunk_count += 1\n",
    "            \n",
    "            if hasattr(chunk, 'content') and chunk.content:\n",
    "                content = chunk.content\n",
    "                full_response += content\n",
    "                logger.debug(f\"Chunk {chunk_count}: {len(content)} chars\")\n",
    "                yield chunk\n",
    "            else:\n",
    "                yield chunk\n",
    "        \n",
    "        logger.info(f\"‚úÖ Streaming completed: {chunk_count} chunks, {len(full_response)} total chars\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Streaming failed: {type(e).__name__}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "logger.info(\"‚úÖ Streaming response handler implemented\")\n",
    "print(\"‚úÖ Streaming response handler implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LangGraph workflow with comprehensive logging\n",
    "\n",
    "# Define Agent State\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, \"Conversation messages\"]\n",
    "\n",
    "\n",
    "# Node 1: LLM Node with retry logic\n",
    "@retry_with_exponential_backoff(max_retries=3, initial_delay=1.0)\n",
    "async def call_model(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Call the LLM with tools bound.\n",
    "    \"\"\"\n",
    "    logger.info(\"ü§ñ LLM Node: Calling model...\")\n",
    "    logger.debug(f\"Current state messages: {len(state['messages'])}\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Bind tools to LLM\n",
    "    model_with_tools = llm.bind_tools(tools)\n",
    "    logger.debug(f\"Tools bound: {[t.name for t in tools]}\")\n",
    "    \n",
    "    # Invoke LLM\n",
    "    logger.info(\"üìû Invoking LLM...\")\n",
    "    response = await model_with_tools.ainvoke(messages)\n",
    "    \n",
    "    logger.info(f\"‚úÖ LLM responded\")\n",
    "    \n",
    "    # Check for tool calls\n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        logger.info(f\"üîß LLM requested {len(response.tool_calls)} tool call(s)\")\n",
    "        for tc in response.tool_calls:\n",
    "            logger.debug(f\"  Tool: {tc.get('name', 'unknown')} with args: {tc.get('args', {})}\")\n",
    "    else:\n",
    "        logger.info(\"üí¨ LLM provided final response (no tool calls)\")\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Router: Decide whether to continue or end\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Determine if we should call tools or end the conversation.\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÄ Router: Determining next step...\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if there are tool calls\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        logger.info(f\"‚û°Ô∏è  Router decision: TOOLS ({len(last_message.tool_calls)} tool calls pending)\")\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        logger.info(\"‚û°Ô∏è  Router decision: END (no tool calls, conversation complete)\")\n",
    "        return \"__end__\"\n",
    "\n",
    "\n",
    "# Create the tool node\n",
    "logger.info(\"Creating ToolNode...\")\n",
    "tool_node = ToolNode(tools)\n",
    "logger.info(f\"ToolNode created with {len(tools)} tools\")\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "logger.info(\"üèóÔ∏è  Building LangGraph workflow...\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "logger.info(\"Adding nodes to graph...\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "logger.info(\"Nodes added: agent, tools\")\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "logger.info(\"Entry point set: agent\")\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "logger.info(\"Conditional edges added: agent -> [tools, END]\")\n",
    "\n",
    "# Add edge from tools back to agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "logger.info(\"Edge added: tools -> agent\")\n",
    "\n",
    "# Compile the graph\n",
    "logger.info(\"Compiling graph...\")\n",
    "graph = workflow.compile()\n",
    "logger.info(\"‚úÖ LangGraph workflow compiled successfully\")\n",
    "\n",
    "print(\"‚úÖ LangGraph Travel Assistant built successfully!\")\n",
    "print(\"Graph structure: START -> agent -> [tools -> agent (loop)] -> END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FastAPI app and endpoint with comprehensive logging\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"Travel Assistant API\", version=\"1.0.0\")\n",
    "logger.info(\"FastAPI app created\")\n",
    "\n",
    "\n",
    "# Request model\n",
    "class TravelRequest(BaseModel):\n",
    "    query: str\n",
    "    stream: bool = True\n",
    "\n",
    "\n",
    "# Response model\n",
    "class TravelResponse(BaseModel):\n",
    "    response: str\n",
    "    status: str\n",
    "\n",
    "\n",
    "@app.post(\"/travel-assistant\")\n",
    "async def travel_assistant_endpoint(request: TravelRequest):\n",
    "    \"\"\"\n",
    "    Travel Assistant endpoint - plans trips using LangGraph and Gemini.\n",
    "    \"\"\"\n",
    "    request_id = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"üåç NEW REQUEST [{request_id}]\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Query: {request.query}\")\n",
    "    logger.info(f\"Streaming: {request.stream}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize state\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=request.query)]\n",
    "        }\n",
    "        logger.info(\"Initial state created\")\n",
    "        \n",
    "        if request.stream:\n",
    "            # Streaming response\n",
    "            logger.info(\"üì° Starting streaming response...\")\n",
    "            \n",
    "            async def event_generator():\n",
    "                try:\n",
    "                    step_count = 0\n",
    "                    \n",
    "                    async for event in graph.astream(initial_state, stream_mode=\"values\"):\n",
    "                        step_count += 1\n",
    "                        logger.debug(f\"Stream step {step_count}\")\n",
    "                        \n",
    "                        messages = event.get(\"messages\", [])\n",
    "                        if messages:\n",
    "                            last_message = messages[-1]\n",
    "                            \n",
    "                            # Stream content\n",
    "                            if hasattr(last_message, 'content') and last_message.content:\n",
    "                                content = last_message.content\n",
    "                                logger.debug(f\"Streaming content: {len(content)} chars\")\n",
    "                                yield f\"data: {json.dumps({'type': 'content', 'data': content})}\\\\n\\\\n\"\n",
    "                            \n",
    "                            # Stream tool calls\n",
    "                            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "                                for tc in last_message.tool_calls:\n",
    "                                    tool_info = {\n",
    "                                        'type': 'tool_call',\n",
    "                                        'tool': tc.get('name', 'unknown'),\n",
    "                                        'args': tc.get('args', {})\n",
    "                                    }\n",
    "                                    logger.info(f\"Streaming tool call: {tool_info['tool']}\")\n",
    "                                    yield f\"data: {json.dumps(tool_info)}\\\\n\\\\n\"\n",
    "                    \n",
    "                    logger.info(f\"‚úÖ Streaming completed: {step_count} steps\")\n",
    "                    yield f\"data: {json.dumps({'type': 'done', 'status': 'success'})}\\\\n\\\\n\"\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"‚ùå Streaming error [{request_id}]: {type(e).__name__}: {str(e)}\")\n",
    "                    error_data = {\n",
    "                        'type': 'error',\n",
    "                        'error': str(e),\n",
    "                        'error_type': type(e).__name__\n",
    "                    }\n",
    "                    yield f\"data: {json.dumps(error_data)}\\\\n\\\\n\"\n",
    "            \n",
    "            return StreamingResponse(\n",
    "                event_generator(),\n",
    "                media_type=\"text/event-stream\",\n",
    "                headers={\n",
    "                    \"Cache-Control\": \"no-cache\",\n",
    "                    \"Connection\": \"keep-alive\",\n",
    "                    \"X-Request-ID\": request_id\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # Non-streaming response\n",
    "            logger.info(\"üîÑ Running non-streaming workflow...\")\n",
    "            \n",
    "            result = await graph.ainvoke(initial_state)\n",
    "            messages = result.get(\"messages\", [])\n",
    "            \n",
    "            if messages:\n",
    "                final_message = messages[-1]\n",
    "                response_content = final_message.content if hasattr(final_message, 'content') else str(final_message)\n",
    "                \n",
    "                logger.info(f\"‚úÖ Request completed [{request_id}]: {len(response_content)} chars\")\n",
    "                \n",
    "                return TravelResponse(\n",
    "                    response=response_content,\n",
    "                    status=\"success\"\n",
    "                )\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è  No messages in result [{request_id}]\")\n",
    "                return TravelResponse(\n",
    "                    response=\"No response generated\",\n",
    "                    status=\"error\"\n",
    "                )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Request failed [{request_id}]: {type(e).__name__}: {str(e)}\")\n",
    "        logger.exception(\"Full traceback:\")\n",
    "        \n",
    "        return TravelResponse(\n",
    "            response=f\"Error: {str(e)}\",\n",
    "            status=\"error\"\n",
    "        )\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    logger.info(\"Health check requested\")\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"service\": \"Travel Assistant API\",\n",
    "        \"version\": \"1.0.0\"\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Detailed health check.\"\"\"\n",
    "    logger.info(\"Detailed health check requested\")\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"tools\": [t.name for t in tools],\n",
    "        \"model\": MODEL_NAME\n",
    "    }\n",
    "\n",
    "\n",
    "logger.info(\"‚úÖ FastAPI endpoints configured\")\n",
    "logger.info(\"Available endpoints: /travel-assistant, /, /health\")\n",
    "\n",
    "print(\"‚úÖ FastAPI endpoint '/travel-assistant' created successfully!\")\n",
    "print(\"\\\\nEndpoints:\")\n",
    "print(\"  POST /travel-assistant - Main travel planning endpoint\")\n",
    "print(\"  GET  /                 - Health check\")\n",
    "print(\"  GET  /health           - Detailed health status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4caa517",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Rubric (Total 20 Points)\n",
    "### **1. Tool Implementation (4 pts)**\n",
    "- ‚úÖ Tools implemented correctly (2 pts)\n",
    "- ‚úÖ Realistic mock responses (2 pts)\n",
    "\n",
    "### **2. Retry Logic (4 pts)**\n",
    "- ‚úÖ Exponential backoff implemented (2 pts)\n",
    "- ‚úÖ Retries trigger correctly (2 pts)\n",
    "\n",
    "### **3. Streaming Responses (4 pts)**\n",
    "- ‚úÖ Streaming implemented (2 pts)\n",
    "- ‚úÖ Smooth incremental output (2 pts)\n",
    "\n",
    "### **4. LangGraph Workflow (4 pts)**\n",
    "- ‚úÖ Graph nodes defined (2 pts)\n",
    "- ‚úÖ Correct tool routing (2 pts)\n",
    "\n",
    "### **5. FastAPI Endpoint (4 pts)**\n",
    "- ‚úÖ Endpoint functional (2 pts)\n",
    "- ‚úÖ Runs graph + streams output (2 pts)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Logging Features Implemented\n",
    "\n",
    "This implementation includes comprehensive logging at every level:\n",
    "\n",
    "1. **Setup Logging**: Initialization and configuration tracking\n",
    "2. **Tool Logging**: Every tool call with parameters and results (üõ´ üå§Ô∏è üóº)\n",
    "3. **Retry Logging**: Attempt counters, backoff delays, success/failure tracking\n",
    "4. **Streaming Logging**: Chunk delivery and progress monitoring\n",
    "5. **Graph Logging**: Node execution, router decisions, state transitions\n",
    "6. **API Logging**: Request IDs, queries, responses, errors\n",
    "7. **Error Logging**: Full exception details with context\n",
    "\n",
    "**Log Output**: \n",
    "- Console (real-time)\n",
    "- File: `travel_assistant.log`\n",
    "\n",
    "**Log Levels**:\n",
    "- INFO: General flow and important events\n",
    "- DEBUG: Detailed execution information\n",
    "- WARNING: Retry attempts and recoverable issues\n",
    "- ERROR: Failures and exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e880110",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Sample Input\n",
    "```\n",
    "Plan a 3-day trip to Tokyo. I need flight options from Singapore, weather forecast, and top attractions.\n",
    "```\n",
    "## ‚úÖ Expected Output (High-Level)\n",
    "```\n",
    "Flights Found:\n",
    "- Singapore ‚Üí Tokyo, $450, 7 AM\n",
    "\n",
    "Weather Forecast:\n",
    "- Day 1: Sunny\n",
    "- Day 2: Cloudy\n",
    "\n",
    "Top Attractions:\n",
    "- Shibuya Crossing\n",
    "- Senso-ji Temple\n",
    "- Tokyo Skytree\n",
    "\n",
    "Suggested Itinerary:\n",
    "Day 1: Shinjuku, Shibuya\n",
    "Day 2: Asakusa, Skytree\n",
    "Day 3: Odaiba\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the FastAPI server\n",
    "# Note: This will run the server - use Ctrl+C to stop\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"üöÄ STARTING FASTAPI SERVER\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Starting FastAPI Server\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\\\nServer will start on: http://127.0.0.1:8000\")\n",
    "print(\"\\\\nEndpoints:\")\n",
    "print(\"  POST http://127.0.0.1:8000/travel-assistant\")\n",
    "print(\"  GET  http://127.0.0.1:8000/\")\n",
    "print(\"  GET  http://127.0.0.1:8000/health\")\n",
    "print(\"\\\\nPress Ctrl+C to stop the server\")\n",
    "print(\"=\"*60 + \"\\\\n\")\n",
    "\n",
    "# Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e649e37",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Start FastAPI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Travel Assistant directly (without FastAPI)\n",
    "\n",
    "async def test_travel_assistant(query: str):\n",
    "    \"\"\"\n",
    "    Test the travel assistant with a query.\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"üß™ TEST MODE\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Test query: {query}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"üß™ Testing Travel Assistant\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\\\nQuery: {query}\\\\n\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Create initial state\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=query)]\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    logger.info(\"Starting graph execution...\")\n",
    "    result = await graph.ainvoke(initial_state)\n",
    "    \n",
    "    # Extract final response\n",
    "    messages = result.get(\"messages\", [])\n",
    "    \n",
    "    logger.info(f\"Graph execution completed: {len(messages)} messages\")\n",
    "    \n",
    "    print(\"\\\\nüìù Response:\\\\n\")\n",
    "    \n",
    "    for msg in messages:\n",
    "        if hasattr(msg, 'content') and msg.content:\n",
    "            print(msg.content)\n",
    "            print()\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    logger.info(\"‚úÖ Test completed\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Run test with the sample input from the assignment\n",
    "test_query = \"Plan a 3-day trip to Tokyo. I need flight options from Singapore, weather forecast, and top attractions.\"\n",
    "\n",
    "logger.info(\"Initiating test...\")\n",
    "test_result = await test_travel_assistant(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72ce3d",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Test the Travel Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612bb1db",
   "metadata": {},
   "source": [
    "---\n",
    "## üåê Task 5 ‚Äî Build FastAPI Endpoint `/travel-assistant`\n",
    "The endpoint must:\n",
    "- Accept user input\n",
    "- Run LangGraph workflow\n",
    "- Stream output to the client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847eb1b7",
   "metadata": {},
   "source": [
    "---\n",
    "## üß© Task 4 ‚Äî Build LangGraph Travel Assistant\n",
    "Implement the full graph:\n",
    "- LLM node\n",
    "- Tool invocation nodes\n",
    "- Router logic\n",
    "- State updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e259ebf",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Task 3 ‚Äî Add Streaming Responses\n",
    "Use Gemini's streaming capability and return partial responses incrementally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a9e7f8",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÅ Task 2 ‚Äî Implement Retry Logic (Exponential Backoff)\n",
    "Your LLM calls must:\n",
    "- Retry on transient errors\n",
    "- Use exponential backoff (1s, 2s, 4s, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce261eec",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Task 1 ‚Äî Implement Tools\n",
    "Build three tools used by the Travel Assistant:\n",
    "### 1. `search_flights`\n",
    "### 2. `get_weather`\n",
    "### 3. `find_attractions`\n",
    "Each tool should return **mock responses**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5985fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, Annotated, Literal, List, Dict, Any\n",
    "from functools import wraps\n",
    "\n",
    "# Google AI\n",
    "import google.generativeai as genai\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# FastAPI\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested async (for Jupyter notebooks)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('travel_assistant.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"üöÄ Travel Assistant Application Starting\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dbf59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q google-generativeai langgraph langchain langchain-google-genai fastapi uvicorn python-dotenv nest-asyncio pydantic tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38764492",
   "metadata": {},
   "source": [
    "## üì¶ Setup\n",
    "Create required imports, install libraries, and configure Gemini API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travel-assistant-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
