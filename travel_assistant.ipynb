{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9126ad5",
   "metadata": {},
   "source": [
    "## üì¶ Setup (Student to Fill)\n",
    "Create required imports, install libraries, and configure Gemini API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e03549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q google-generativeai langgraph langchain langchain-google-genai fastapi uvicorn python-dotenv nest-asyncio pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, Annotated, Literal, List, Dict, Any\n",
    "from functools import wraps\n",
    "\n",
    "# Google AI\n",
    "import google.generativeai as genai\n",
    "from google.api_core import exceptions as google_exceptions\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# FastAPI\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import StreamingResponse\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "# Enable nested async (for Jupyter notebooks)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('travel_assistant.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"=\"*50)\n",
    "logger.info(\"Travel Assistant Application Starting\")\n",
    "logger.info(\"=\"*50)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "logger.info(\"Environment variables loaded\")\n",
    "\n",
    "# Configure Gemini API\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    logger.error(\"GOOGLE_API_KEY not found in environment variables\")\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found. Please set it in .env file\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "logger.info(\"Gemini API configured successfully\")\n",
    "\n",
    "# Initialize Gemini model\n",
    "MODEL_NAME = \"gemini-1.5-flash\"\n",
    "logger.info(f\"Initializing Gemini model: {MODEL_NAME}\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.7,\n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "logger.info(\"Setup completed successfully\")\n",
    "print(\"‚úÖ Setup complete! Gemini API configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279108b4",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Task 1 ‚Äî Implement Tools\n",
    "Build three tools used by the Travel Assistant:\n",
    "### 1. `search_flights`\n",
    "### 2. `get_weather`\n",
    "### 3. `find_attractions`\n",
    "Each tool should return **mock responses**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fab30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement mock tools with comprehensive logging\n",
    "\n",
    "@tool\n",
    "def search_flights(origin: str, destination: str, date: str = \"2025-12-01\") -> dict:\n",
    "    \"\"\"\n",
    "    Search for flight options between origin and destination.\n",
    "    \n",
    "    Args:\n",
    "        origin: Departure city\n",
    "        destination: Arrival city\n",
    "        date: Travel date (YYYY-MM-DD format)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing flight options\n",
    "    \"\"\"\n",
    "    logger.info(f\"üõ´ search_flights called: {origin} ‚Üí {destination} on {date}\")\n",
    "    \n",
    "    # Mock flight data\n",
    "    mock_flights = {\n",
    "        \"flights\": [\n",
    "            {\n",
    "                \"airline\": \"Singapore Airlines\",\n",
    "                \"flight_number\": \"SQ638\",\n",
    "                \"price_usd\": 450,\n",
    "                \"departure_time\": \"07:00 AM\",\n",
    "                \"arrival_time\": \"02:30 PM\",\n",
    "                \"duration\": \"6h 30m\",\n",
    "                \"stops\": \"Direct\"\n",
    "            },\n",
    "            {\n",
    "                \"airline\": \"ANA\",\n",
    "                \"flight_number\": \"NH842\",\n",
    "                \"price_usd\": 420,\n",
    "                \"departure_time\": \"11:30 AM\",\n",
    "                \"arrival_time\": \"07:00 PM\",\n",
    "                \"duration\": \"6h 30m\",\n",
    "                \"stops\": \"Direct\"\n",
    "            },\n",
    "            {\n",
    "                \"airline\": \"JAL\",\n",
    "                \"flight_number\": \"JL712\",\n",
    "                \"price_usd\": 480,\n",
    "                \"departure_time\": \"09:15 AM\",\n",
    "                \"arrival_time\": \"04:45 PM\",\n",
    "                \"duration\": \"6h 30m\",\n",
    "                \"stops\": \"Direct\"\n",
    "            }\n",
    "        ],\n",
    "        \"origin\": origin,\n",
    "        \"destination\": destination,\n",
    "        \"date\": date\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"‚úÖ Found {len(mock_flights['flights'])} flights from {origin} to {destination}\")\n",
    "    logger.debug(f\"Flight data: {json.dumps(mock_flights, indent=2)}\")\n",
    "    \n",
    "    return mock_flights\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str, days: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Get weather forecast for a location.\n",
    "    \n",
    "    Args:\n",
    "        location: City name\n",
    "        days: Number of days for forecast (1-7)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing weather forecast\n",
    "    \"\"\"\n",
    "    logger.info(f\"üå§Ô∏è get_weather called: {location} for {days} days\")\n",
    "    \n",
    "    # Mock weather conditions\n",
    "    conditions = [\"Sunny\", \"Partly Cloudy\", \"Cloudy\", \"Light Rain\", \"Clear\"]\n",
    "    \n",
    "    forecast = {\n",
    "        \"location\": location,\n",
    "        \"forecast\": []\n",
    "    }\n",
    "    \n",
    "    for i in range(min(days, 7)):\n",
    "        day_forecast = {\n",
    "            \"day\": f\"Day {i + 1}\",\n",
    "            \"condition\": conditions[i % len(conditions)],\n",
    "            \"temperature_celsius\": 22 + (i % 5),\n",
    "            \"humidity_percent\": 55 + (i * 5),\n",
    "            \"precipitation_chance\": 10 + (i * 5)\n",
    "        }\n",
    "        forecast[\"forecast\"].append(day_forecast)\n",
    "    \n",
    "    logger.info(f\"‚úÖ Weather forecast retrieved for {location} ({days} days)\")\n",
    "    logger.debug(f\"Weather data: {json.dumps(forecast, indent=2)}\")\n",
    "    \n",
    "    return forecast\n",
    "\n",
    "\n",
    "@tool\n",
    "def find_attractions(location: str, limit: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Find top tourist attractions in a location.\n",
    "    \n",
    "    Args:\n",
    "        location: City name\n",
    "        limit: Maximum number of attractions to return\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing attraction information\n",
    "    \"\"\"\n",
    "    logger.info(f\"üóº find_attractions called: {location} (limit: {limit})\")\n",
    "    \n",
    "    # Mock attractions data\n",
    "    attractions_db = {\n",
    "        \"Tokyo\": [\n",
    "            {\"name\": \"Shibuya Crossing\", \"type\": \"Landmark\", \"rating\": 4.8, \"description\": \"Famous scramble crossing\"},\n",
    "            {\"name\": \"Senso-ji Temple\", \"type\": \"Temple\", \"rating\": 4.7, \"description\": \"Ancient Buddhist temple in Asakusa\"},\n",
    "            {\"name\": \"Tokyo Skytree\", \"type\": \"Observation Tower\", \"rating\": 4.6, \"description\": \"Tallest structure in Japan\"},\n",
    "            {\"name\": \"Meiji Shrine\", \"type\": \"Shrine\", \"rating\": 4.7, \"description\": \"Shinto shrine in forest setting\"},\n",
    "            {\"name\": \"Tokyo Tower\", \"type\": \"Landmark\", \"rating\": 4.5, \"description\": \"Iconic communications tower\"},\n",
    "            {\"name\": \"Tsukiji Outer Market\", \"type\": \"Market\", \"rating\": 4.6, \"description\": \"Fresh seafood and street food\"},\n",
    "        ],\n",
    "        \"default\": [\n",
    "            {\"name\": \"City Center\", \"type\": \"District\", \"rating\": 4.5, \"description\": \"Main downtown area\"},\n",
    "            {\"name\": \"Historic Quarter\", \"type\": \"District\", \"rating\": 4.6, \"description\": \"Old town with traditional architecture\"},\n",
    "            {\"name\": \"Central Park\", \"type\": \"Park\", \"rating\": 4.4, \"description\": \"Large urban park\"},\n",
    "            {\"name\": \"National Museum\", \"type\": \"Museum\", \"rating\": 4.7, \"description\": \"Cultural and historical exhibits\"},\n",
    "            {\"name\": \"Waterfront\", \"type\": \"Area\", \"rating\": 4.5, \"description\": \"Scenic riverside or harbor area\"},\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Get attractions for location or use default\n",
    "    attractions_list = attractions_db.get(location, attractions_db[\"default\"])\n",
    "    limited_attractions = attractions_list[:limit]\n",
    "    \n",
    "    result = {\n",
    "        \"location\": location,\n",
    "        \"attractions\": limited_attractions,\n",
    "        \"total_found\": len(limited_attractions)\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"‚úÖ Found {len(limited_attractions)} attractions in {location}\")\n",
    "    logger.debug(f\"Attractions data: {json.dumps(result, indent=2)}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Register tools\n",
    "tools = [search_flights, get_weather, find_attractions]\n",
    "logger.info(f\"Registered {len(tools)} tools: {[t.name for t in tools]}\")\n",
    "\n",
    "print(\"‚úÖ Tools implemented successfully!\")\n",
    "print(f\"Available tools: {[t.name for t in tools]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a17be",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÅ Task 2 ‚Äî Implement Retry Logic (Exponential Backoff)\n",
    "Your LLM calls must:\n",
    "- Retry on transient errors\n",
    "- Use exponential backoff (1s, 2s, 4s, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement retry logic wrapper with comprehensive logging\n",
    "\n",
    "def retry_with_exponential_backoff(\n",
    "    max_retries: int = 3,\n",
    "    initial_delay: float = 1.0,\n",
    "    exponential_base: float = 2.0,\n",
    "    max_delay: float = 60.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Decorator for retrying a function with exponential backoff.\n",
    "    \n",
    "    Args:\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        initial_delay: Initial delay in seconds\n",
    "        exponential_base: Base for exponential calculation\n",
    "        max_delay: Maximum delay between retries\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        async def async_wrapper(*args, **kwargs):\n",
    "            retry_count = 0\n",
    "            \n",
    "            while retry_count <= max_retries:\n",
    "                try:\n",
    "                    logger.debug(f\"Attempting {func.__name__} (attempt {retry_count + 1}/{max_retries + 1})\")\n",
    "                    result = await func(*args, **kwargs)\n",
    "                    \n",
    "                    if retry_count > 0:\n",
    "                        logger.info(f\"‚úÖ {func.__name__} succeeded after {retry_count} retries\")\n",
    "                    \n",
    "                    return result\n",
    "                    \n",
    "                except (google_exceptions.ResourceExhausted,\n",
    "                        google_exceptions.ServiceUnavailable,\n",
    "                        google_exceptions.DeadlineExceeded,\n",
    "                        ConnectionError,\n",
    "                        TimeoutError) as e:\n",
    "                    \n",
    "                    retry_count += 1\n",
    "                    \n",
    "                    if retry_count > max_retries:\n",
    "                        logger.error(f\"‚ùå {func.__name__} failed after {max_retries} retries: {str(e)}\")\n",
    "                        raise\n",
    "                    \n",
    "                    # Calculate delay with exponential backoff\n",
    "                    delay = min(initial_delay * (exponential_base ** (retry_count - 1)), max_delay)\n",
    "                    \n",
    "                    logger.warning(\n",
    "                        f\"‚ö†Ô∏è {func.__name__} failed (attempt {retry_count}/{max_retries + 1}): {type(e).__name__}\"\n",
    "                    )\n",
    "                    logger.info(f\"üîÑ Retrying in {delay:.2f} seconds...\")\n",
    "                    \n",
    "                    await asyncio.sleep(delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Don't retry on non-transient errors\n",
    "                    logger.error(f\"‚ùå {func.__name__} failed with non-retryable error: {type(e).__name__}: {str(e)}\")\n",
    "                    raise\n",
    "            \n",
    "            logger.error(f\"‚ùå {func.__name__} exceeded maximum retries\")\n",
    "            raise Exception(f\"Maximum retries ({max_retries}) exceeded for {func.__name__}\")\n",
    "        \n",
    "        @wraps(func)\n",
    "        def sync_wrapper(*args, **kwargs):\n",
    "            retry_count = 0\n",
    "            \n",
    "            while retry_count <= max_retries:\n",
    "                try:\n",
    "                    logger.debug(f\"Attempting {func.__name__} (attempt {retry_count + 1}/{max_retries + 1})\")\n",
    "                    result = func(*args, **kwargs)\n",
    "                    \n",
    "                    if retry_count > 0:\n",
    "                        logger.info(f\"‚úÖ {func.__name__} succeeded after {retry_count} retries\")\n",
    "                    \n",
    "                    return result\n",
    "                    \n",
    "                except (google_exceptions.ResourceExhausted,\n",
    "                        google_exceptions.ServiceUnavailable,\n",
    "                        google_exceptions.DeadlineExceeded,\n",
    "                        ConnectionError,\n",
    "                        TimeoutError) as e:\n",
    "                    \n",
    "                    retry_count += 1\n",
    "                    \n",
    "                    if retry_count > max_retries:\n",
    "                        logger.error(f\"‚ùå {func.__name__} failed after {max_retries} retries: {str(e)}\")\n",
    "                        raise\n",
    "                    \n",
    "                    # Calculate delay with exponential backoff\n",
    "                    delay = min(initial_delay * (exponential_base ** (retry_count - 1)), max_delay)\n",
    "                    \n",
    "                    logger.warning(\n",
    "                        f\"‚ö†Ô∏è {func.__name__} failed (attempt {retry_count}/{max_retries + 1}): {type(e).__name__}\"\n",
    "                    )\n",
    "                    logger.info(f\"üîÑ Retrying in {delay:.2f} seconds...\")\n",
    "                    \n",
    "                    time.sleep(delay)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Don't retry on non-transient errors\n",
    "                    logger.error(f\"‚ùå {func.__name__} failed with non-retryable error: {type(e).__name__}: {str(e)}\")\n",
    "                    raise\n",
    "            \n",
    "            logger.error(f\"‚ùå {func.__name__} exceeded maximum retries\")\n",
    "            raise Exception(f\"Maximum retries ({max_retries}) exceeded for {func.__name__}\")\n",
    "        \n",
    "        # Return appropriate wrapper based on function type\n",
    "        if asyncio.iscoroutinefunction(func):\n",
    "            return async_wrapper\n",
    "        else:\n",
    "            return sync_wrapper\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "\n",
    "logger.info(\"Retry logic with exponential backoff implemented\")\n",
    "print(\"‚úÖ Retry logic implemented with exponential backoff (1s, 2s, 4s, 8s...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76695fdd",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Task 3 ‚Äî Add Streaming Responses\n",
    "Use Gemini's streaming capability and return partial responses incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25434f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement streaming response handler with comprehensive logging\n",
    "\n",
    "async def stream_llm_response(messages: list, tools_list: list = None):\n",
    "    \"\"\"\n",
    "    Stream responses from Gemini LLM.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of conversation messages\n",
    "        tools_list: Optional list of tools to bind\n",
    "    \n",
    "    Yields:\n",
    "        Partial response chunks\n",
    "    \"\"\"\n",
    "    logger.info(\"üì° Starting streaming LLM response\")\n",
    "    logger.debug(f\"Input messages count: {len(messages)}\")\n",
    "    \n",
    "    try:\n",
    "        # Bind tools if provided\n",
    "        model = llm\n",
    "        if tools_list:\n",
    "            model = llm.bind_tools(tools_list)\n",
    "            logger.debug(f\"Tools bound to model: {[t.name for t in tools_list]}\")\n",
    "        \n",
    "        # Stream the response\n",
    "        chunk_count = 0\n",
    "        full_response = \"\"\n",
    "        \n",
    "        logger.info(\"üöÄ Invoking LLM with streaming...\")\n",
    "        async for chunk in model.astream(messages):\n",
    "            chunk_count += 1\n",
    "            \n",
    "            if hasattr(chunk, 'content') and chunk.content:\n",
    "                content = chunk.content\n",
    "                full_response += content\n",
    "                logger.debug(f\"Chunk {chunk_count}: {len(content)} chars\")\n",
    "                yield chunk\n",
    "            else:\n",
    "                yield chunk\n",
    "        \n",
    "        logger.info(f\"‚úÖ Streaming completed: {chunk_count} chunks, {len(full_response)} total chars\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Streaming failed: {type(e).__name__}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "logger.info(\"Streaming response handler implemented\")\n",
    "print(\"‚úÖ Streaming response handler implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d820b",
   "metadata": {},
   "source": [
    "---\n",
    "## üß© Task 4 ‚Äî Build LangGraph Travel Assistant\n",
    "Implement the full graph:\n",
    "- LLM node\n",
    "- Tool invocation nodes\n",
    "- Router logic\n",
    "- State updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LangGraph workflow with comprehensive logging\n",
    "\n",
    "# Define Agent State\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, \"Conversation messages\"]\n",
    "\n",
    "\n",
    "# Node 1: LLM Node with retry logic\n",
    "@retry_with_exponential_backoff(max_retries=3, initial_delay=1.0)\n",
    "async def call_model(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Call the LLM with tools bound.\n",
    "    \"\"\"\n",
    "    logger.info(\"ü§ñ LLM Node: Calling model...\")\n",
    "    logger.debug(f\"Current state messages: {len(state['messages'])}\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    # Bind tools to LLM\n",
    "    model_with_tools = llm.bind_tools(tools)\n",
    "    logger.debug(f\"Tools bound: {[t.name for t in tools]}\")\n",
    "    \n",
    "    # Invoke LLM\n",
    "    logger.info(\"üìû Invoking LLM...\")\n",
    "    response = await model_with_tools.ainvoke(messages)\n",
    "    \n",
    "    logger.info(f\"‚úÖ LLM responded\")\n",
    "    \n",
    "    # Check for tool calls\n",
    "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "        logger.info(f\"üîß LLM requested {len(response.tool_calls)} tool call(s)\")\n",
    "        for tc in response.tool_calls:\n",
    "            logger.debug(f\"  Tool: {tc.get('name', 'unknown')} with args: {tc.get('args', {})}\")\n",
    "    else:\n",
    "        logger.info(\"üí¨ LLM provided final response (no tool calls)\")\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Router: Decide whether to continue or end\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Determine if we should call tools or end the conversation.\n",
    "    \"\"\"\n",
    "    logger.info(\"üîÄ Router: Determining next step...\")\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Check if there are tool calls\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        logger.info(f\"‚û°Ô∏è Router decision: TOOLS ({len(last_message.tool_calls)} tool calls pending)\")\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        logger.info(\"‚û°Ô∏è Router decision: END (no tool calls, conversation complete)\")\n",
    "        return \"__end__\"\n",
    "\n",
    "\n",
    "# Create the tool node\n",
    "logger.info(\"Creating ToolNode...\")\n",
    "tool_node = ToolNode(tools)\n",
    "logger.info(f\"ToolNode created with {len(tools)} tools\")\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "logger.info(\"üèóÔ∏è Building LangGraph workflow...\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "logger.info(\"Adding nodes to graph...\")\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "logger.info(\"Nodes added: agent, tools\")\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"agent\")\n",
    "logger.info(\"Entry point set: agent\")\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "logger.info(\"Conditional edges added: agent -> [tools, END]\")\n",
    "\n",
    "# Add edge from tools back to agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "logger.info(\"Edge added: tools -> agent\")\n",
    "\n",
    "# Compile the graph\n",
    "logger.info(\"Compiling graph...\")\n",
    "graph = workflow.compile()\n",
    "logger.info(\"‚úÖ LangGraph workflow compiled successfully\")\n",
    "\n",
    "print(\"‚úÖ LangGraph Travel Assistant built successfully!\")\n",
    "print(\"Graph structure: START -> agent -> [tools -> agent (loop)] -> END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec59c3ad",
   "metadata": {},
   "source": [
    "---\n",
    "## üåê Task 5 ‚Äî Build FastAPI Endpoint `/travel-assistant`\n",
    "The endpoint must:\n",
    "- Accept user input\n",
    "- Run LangGraph workflow\n",
    "- Stream output to the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db0cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FastAPI app and endpoint with comprehensive logging\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(title=\"Travel Assistant API\", version=\"1.0.0\")\n",
    "logger.info(\"FastAPI app created\")\n",
    "\n",
    "\n",
    "# Request model\n",
    "class TravelRequest(BaseModel):\n",
    "    query: str\n",
    "    stream: bool = True\n",
    "\n",
    "\n",
    "# Response model\n",
    "class TravelResponse(BaseModel):\n",
    "    response: str\n",
    "    status: str\n",
    "\n",
    "\n",
    "@app.post(\"/travel-assistant\")\n",
    "async def travel_assistant_endpoint(request: TravelRequest):\n",
    "    \"\"\"\n",
    "    Travel Assistant endpoint - plans trips using LangGraph and Gemini.\n",
    "    \"\"\"\n",
    "    request_id = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"üåç NEW REQUEST [{request_id}]\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Query: {request.query}\")\n",
    "    logger.info(f\"Streaming: {request.stream}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize state\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=request.query)]\n",
    "        }\n",
    "        logger.info(\"Initial state created\")\n",
    "        \n",
    "        if request.stream:\n",
    "            # Streaming response\n",
    "            logger.info(\"üì° Starting streaming response...\")\n",
    "            \n",
    "            async def event_generator():\n",
    "                try:\n",
    "                    step_count = 0\n",
    "                    \n",
    "                    async for event in graph.astream(initial_state, stream_mode=\"values\"):\n",
    "                        step_count += 1\n",
    "                        logger.debug(f\"Stream step {step_count}\")\n",
    "                        \n",
    "                        messages = event.get(\"messages\", [])\n",
    "                        if messages:\n",
    "                            last_message = messages[-1]\n",
    "                            \n",
    "                            # Stream content\n",
    "                            if hasattr(last_message, 'content') and last_message.content:\n",
    "                                content = last_message.content\n",
    "                                logger.debug(f\"Streaming content: {len(content)} chars\")\n",
    "                                yield f\"data: {json.dumps({'type': 'content', 'data': content})}\\n\\n\"\n",
    "                            \n",
    "                            # Stream tool calls\n",
    "                            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "                                for tc in last_message.tool_calls:\n",
    "                                    tool_info = {\n",
    "                                        'type': 'tool_call',\n",
    "                                        'tool': tc.get('name', 'unknown'),\n",
    "                                        'args': tc.get('args', {})\n",
    "                                    }\n",
    "                                    logger.info(f\"Streaming tool call: {tool_info['tool']}\")\n",
    "                                    yield f\"data: {json.dumps(tool_info)}\\n\\n\"\n",
    "                    \n",
    "                    logger.info(f\"‚úÖ Streaming completed: {step_count} steps\")\n",
    "                    yield f\"data: {json.dumps({'type': 'done', 'status': 'success'})}\\n\\n\"\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"‚ùå Streaming error [{request_id}]: {type(e).__name__}: {str(e)}\")\n",
    "                    error_data = {\n",
    "                        'type': 'error',\n",
    "                        'error': str(e),\n",
    "                        'error_type': type(e).__name__\n",
    "                    }\n",
    "                    yield f\"data: {json.dumps(error_data)}\\n\\n\"\n",
    "            \n",
    "            return StreamingResponse(\n",
    "                event_generator(),\n",
    "                media_type=\"text/event-stream\",\n",
    "                headers={\n",
    "                    \"Cache-Control\": \"no-cache\",\n",
    "                    \"Connection\": \"keep-alive\",\n",
    "                    \"X-Request-ID\": request_id\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            # Non-streaming response\n",
    "            logger.info(\"üîÑ Running non-streaming workflow...\")\n",
    "            \n",
    "            result = await graph.ainvoke(initial_state)\n",
    "            messages = result.get(\"messages\", [])\n",
    "            \n",
    "            if messages:\n",
    "                final_message = messages[-1]\n",
    "                response_content = final_message.content if hasattr(final_message, 'content') else str(final_message)\n",
    "                \n",
    "                logger.info(f\"‚úÖ Request completed [{request_id}]: {len(response_content)} chars\")\n",
    "                \n",
    "                return TravelResponse(\n",
    "                    response=response_content,\n",
    "                    status=\"success\"\n",
    "                )\n",
    "            else:\n",
    "                logger.warning(f\"‚ö†Ô∏è No messages in result [{request_id}]\")\n",
    "                return TravelResponse(\n",
    "                    response=\"No response generated\",\n",
    "                    status=\"error\"\n",
    "                )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Request failed [{request_id}]: {type(e).__name__}: {str(e)}\")\n",
    "        logger.exception(\"Full traceback:\")\n",
    "        \n",
    "        return TravelResponse(\n",
    "            response=f\"Error: {str(e)}\",\n",
    "            status=\"error\"\n",
    "        )\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    logger.info(\"Health check requested\")\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"service\": \"Travel Assistant API\",\n",
    "        \"version\": \"1.0.0\"\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Detailed health check.\"\"\"\n",
    "    logger.info(\"Detailed health check requested\")\n",
    "    return {\n",
    "        \"status\": \"healthy\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"tools\": [t.name for t in tools],\n",
    "        \"model\": MODEL_NAME\n",
    "    }\n",
    "\n",
    "\n",
    "logger.info(\"‚úÖ FastAPI endpoints configured\")\n",
    "logger.info(\"Available endpoints: /travel-assistant, /, /health\")\n",
    "\n",
    "print(\"‚úÖ FastAPI endpoint '/travel-assistant' created successfully!\")\n",
    "print(\"\\nEndpoints:\")\n",
    "print(\"  POST /travel-assistant - Main travel planning endpoint\")\n",
    "print(\"  GET  /              - Health check\")\n",
    "print(\"  GET  /health        - Detailed health status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d9bea",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Test the Travel Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a377d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Travel Assistant directly (without FastAPI)\n",
    "\n",
    "async def test_travel_assistant(query: str):\n",
    "    \"\"\"\n",
    "    Test the travel assistant with a query.\n",
    "    \"\"\"\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(\"üß™ TEST MODE\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Test query: {query}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üß™ Testing Travel Assistant\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nQuery: {query}\\n\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Create initial state\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=query)]\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    logger.info(\"Starting graph execution...\")\n",
    "    result = await graph.ainvoke(initial_state)\n",
    "    \n",
    "    # Extract final response\n",
    "    messages = result.get(\"messages\", [])\n",
    "    \n",
    "    logger.info(f\"Graph execution completed: {len(messages)} messages\")\n",
    "    \n",
    "    print(\"\\nüìù Response:\\n\")\n",
    "    \n",
    "    for msg in messages:\n",
    "        if hasattr(msg, 'content') and msg.content:\n",
    "            print(msg.content)\n",
    "            print()\n",
    "    \n",
    "    print(\"-\"*60)\n",
    "    logger.info(\"‚úÖ Test completed\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Run test\n",
    "test_query = \"Plan a 3-day trip to Tokyo. I need flight options from Singapore, weather forecast, and top attractions.\"\n",
    "\n",
    "logger.info(\"Initiating test...\")\n",
    "test_result = await test_travel_assistant(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23df00",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Start FastAPI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2946527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the FastAPI server\n",
    "# Note: This will run the server - use Ctrl+C to stop\n",
    "\n",
    "import uvicorn\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"üöÄ STARTING FASTAPI SERVER\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Starting FastAPI Server\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nServer will start on: http://127.0.0.1:8000\")\n",
    "print(\"\\nEndpoints:\")\n",
    "print(\"  POST http://127.0.0.1:8000/travel-assistant\")\n",
    "print(\"  GET  http://127.0.0.1:8000/\")\n",
    "print(\"  GET  http://127.0.0.1:8000/health\")\n",
    "print(\"\\nPress Ctrl+C to stop the server\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd267e4",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Sample Input\n",
    "```\n",
    "Plan a 3-day trip to Tokyo. I need flight options from Singapore, weather forecast, and top attractions.\n",
    "```\n",
    "## ‚úÖ Expected Output (High-Level)\n",
    "```\n",
    "Flights Found:\n",
    "- Singapore ‚Üí Tokyo, $450, 7 AM\n",
    "\n",
    "Weather Forecast:\n",
    "- Day 1: Sunny\n",
    "- Day 2: Cloudy\n",
    "\n",
    "Top Attractions:\n",
    "- Shibuya Crossing\n",
    "- Senso-ji Temple\n",
    "- Tokyo Skytree\n",
    "\n",
    "Suggested Itinerary:\n",
    "Day 1: Shinjuku, Shibuya\n",
    "Day 2: Asakusa, Skytree\n",
    "Day 3: Odaiba\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360bd30",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Rubric (Total 20 Points)\n",
    "### **1. Tool Implementation (4 pts)**\n",
    "- Tools implemented correctly (2 pts)\n",
    "- Realistic mock responses (2 pts)\n",
    "\n",
    "### **2. Retry Logic (4 pts)**\n",
    "- Exponential backoff implemented (2 pts)\n",
    "- Retries trigger correctly (2 pts)\n",
    "\n",
    "### **3. Streaming Responses (4 pts)**\n",
    "- Streaming implemented (2 pts)\n",
    "- Smooth incremental output (2 pts)\n",
    "\n",
    "### **4. LangGraph Workflow (4 pts)**\n",
    "- Graph nodes defined (2 pts)\n",
    "- Correct tool routing (2 pts)\n",
    "\n",
    "### **5. FastAPI Endpoint (4 pts)**\n",
    "- Endpoint functional (2 pts)\n",
    "- Runs graph + streams output (2 pts)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Logging Features Implemented\n",
    "\n",
    "This implementation includes comprehensive logging:\n",
    "\n",
    "1. **Setup Logging**: Tracks initialization and configuration\n",
    "2. **Tool Logging**: Logs every tool invocation with parameters and results\n",
    "3. **Retry Logging**: Tracks retry attempts, delays, and failures\n",
    "4. **Streaming Logging**: Monitors chunk delivery and streaming progress\n",
    "5. **Graph Logging**: Traces node execution and routing decisions\n",
    "6. **API Logging**: Records all API requests and responses\n",
    "7. **Error Logging**: Captures and logs all errors with full context\n",
    "\n",
    "**Log Output**: \n",
    "- Console (stdout)\n",
    "- File: `travel_assistant.log`\n",
    "\n",
    "**Log Levels**:\n",
    "- INFO: General flow and important events\n",
    "- DEBUG: Detailed execution information\n",
    "- WARNING: Retry attempts and recoverable issues\n",
    "- ERROR: Failures and exceptions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
